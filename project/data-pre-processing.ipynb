{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalize the samples' values to scale them to a smaller, consistent range.\n",
    "\n",
    "It ensures that all features have the same scale, which can help in improving the performance for certain ML algorithms.\n",
    "\n",
    "### Min-Max scaling (x - min) / (max - min)\n",
    "\n",
    "https://www.educative.io/answers/what-is-data-scaling-and-normalization-in-machine-learning\n",
    "\n",
    "- scales the values of a numeric feature into a 0 to 1 range (typically)\n",
    "- preserves the original data distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "balanced_dataset_path = \"./data/4-data-balancing/reduced_dataset.csv\"\n",
    "normalized_dataset_path = \"./data/5-data-scaling/min_max_normalization.csv\"\n",
    "\n",
    "# Load the dataset in a df\n",
    "df = pd.read_csv(balanced_dataset_path)\n",
    "\n",
    "# Define the feature columns selected for Min-max Scaling\n",
    "features_df = [\"empatica_bvp\", \"empatica_eda\", \"empatica_temp\", \"samsung_bvp\"]\n",
    "\n",
    "\n",
    "# Min-Max Scaling implementation\n",
    "def min_max_scaling(x):\n",
    "    min = x.min()\n",
    "    max = x.max()\n",
    "    return ((x - min) / (max - min)).apply(lambda norm: f\"{norm:.10f}\")\n",
    "\n",
    "\n",
    "# Apply min-max scaling to the feature\n",
    "for feature in features_df:\n",
    "    df[feature] = min_max_scaling(df[feature])\n",
    "\n",
    "# Write the Min-Max scaled df into a new .csv file\n",
    "df.to_csv(normalized_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-score normalization (Standardization)\n",
    "\n",
    "https://www.educative.io/answers/what-is-data-scaling-and-normalization-in-machine-learning\n",
    "\n",
    "- calculates the mean (μ) and standard deviation (σ) of each feature column and scale the values using the Z-score formula\n",
    "- aiming to have a mean 0 and a standard deviation 1\n",
    "- useful when the features have different units or different ranges\n",
    "- used in centering the data around zero and scaling it to have unit variance\n",
    "- a z-score close or equal to 0 - the data point is very close or exactly the mean\n",
    "- positive or negative z-scores represents the number of std-s the data point deviates from the mean (higher +; smaller -)\n",
    "\n",
    "Z = (x − μ) / σ\n",
    "\n",
    "Where:\n",
    "\n",
    "- Z : the z-score\n",
    "- x : the data point\n",
    "- μ : the mean\n",
    "- σ : the standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "balanced_dataset_path = \"./data/4-data-balancing/reduced_dataset.csv\"\n",
    "z_score_normalized_dataset_path = \"./data/5-data-scaling/z_score_standardization.csv\"\n",
    "\n",
    "# Load the dataset into a df\n",
    "df = pd.read_csv(balanced_dataset_path)\n",
    "\n",
    "# Define the feature columns selected for Z-score Scaling\n",
    "features_df = [\"empatica_bvp\", \"empatica_eda\", \"empatica_temp\", \"samsung_bvp\"]\n",
    "\n",
    "\n",
    "# Z-score Scaling implementation\n",
    "def z_score_scaling(x):\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    return ((x - mean) / std).apply(lambda stand: f\"{stand:.10f}\")\n",
    "\n",
    "\n",
    "# Apply Z-score scaling to the features\n",
    "for feature in features_df:\n",
    "    df[feature] = z_score_scaling(df[feature])\n",
    "\n",
    "# Write the Z-score scaled df to a new .csv file\n",
    "df.to_csv(z_score_normalized_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data into training, testing, validation datasets\n",
    "\n",
    "- **training**\n",
    "\n",
    "  - the data used for training\n",
    "  - split ID wise\n",
    "  - containing the samples from IDs 0, 2, 4, 6, 8\n",
    "\n",
    "- **testing**\n",
    "  - the data used for testing\n",
    "  - split ID wise\n",
    "  - containing the samples from ID 9\n",
    "- **validation**\n",
    "  - the data used for validation\n",
    "  - split ID wise\n",
    "  - containing the samples from ID 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "min_max_dataset_path = \"./data/5-data-scaling/min_max_normalization.csv\"\n",
    "z_score_dataset_path = \"./data/5-data-scaling/z_score_standardization.csv\"\n",
    "\n",
    "min_max_training_dataset_path = \"./data/6-data-split/min-max/a_training.csv\"\n",
    "min_max_testing_dataset_path = \"./data/6-data-split/min-max/b_testing.csv\"\n",
    "min_max_validation_dataset_path = \"./data/6-data-split/min-max/c_validation.csv\"\n",
    "\n",
    "z_score_training_dataset_path = \"./data/6-data-split/z-score/a_training.csv\"\n",
    "z_score_testing_dataset_path = \"./data/6-data-split/z-score/b_testing.csv\"\n",
    "z_score_validation_dataset_path = \"./data/6-data-split/z-score/c_validation.csv\"\n",
    "\n",
    "# Define a dictionary for the two scaled versions datasets\n",
    "scaled_datasets = {\n",
    "    \"min_max\": {\n",
    "        \"path\": min_max_dataset_path,\n",
    "        \"training_path\": min_max_training_dataset_path,\n",
    "        \"testing_path\": min_max_testing_dataset_path,\n",
    "        \"validation_path\": min_max_validation_dataset_path,\n",
    "    },\n",
    "    \"z_score\": {\n",
    "        \"path\": z_score_dataset_path,\n",
    "        \"training_path\": z_score_training_dataset_path,\n",
    "        \"testing_path\": z_score_testing_dataset_path,\n",
    "        \"validation_path\": z_score_validation_dataset_path,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the IDs for each subset\n",
    "training_ids = [0, 2, 4, 6, 8]\n",
    "testing_id = 9\n",
    "validation_id = 10\n",
    "\n",
    "# Iterate over each of the scaled datasets using `data` for the dictionary data\n",
    "for _, data in scaled_datasets.items():\n",
    "    # Load the input dataset having the path value at key `path`\n",
    "    df = pd.read_csv(data[\"path\"])\n",
    "\n",
    "    # Create corresponding split subsets, ID-wise\n",
    "    training_df = df[df[\"ID\"].isin(training_ids)]\n",
    "    testing_df = df[df[\"ID\"] == testing_id]\n",
    "    validation_df = df[df[\"ID\"] == validation_id]\n",
    "\n",
    "    # Define the list of the dictionaries' path keys\n",
    "    path_keys = [\"training_path\", \"testing_path\", \"validation_path\"]\n",
    "\n",
    "    # Iterate over the subsets to reset the indexes corresponding to the new df structures / content\n",
    "    for i, dataset in enumerate([training_df, testing_df, validation_df]):\n",
    "        dataset.reset_index(drop=True, inplace=True)\n",
    "        dataset.to_csv(data[path_keys[i]], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
