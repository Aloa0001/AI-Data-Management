{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Load and Wearable Integration Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset traversal discovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def visualize_csv(csv_path):\n",
    "#     # Read CSV file\n",
    "#     df = pd.read_csv(csv_path)\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for column in df.columns:\n",
    "#         if column != \"time\":\n",
    "#             plt.plot(df[\"time\"], df[column], label=column)\n",
    "#     plt.xlabel(\"Time\")\n",
    "#     plt.ylabel(\"Value\")\n",
    "#     plt.title(\"Data from {}\".format(os.path.basename(csv_path)))\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def process_directory(root_dir):\n",
    "#     for subdir, dirs, files in os.walk(root_dir):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".csv\") and (\n",
    "#                 \"empatica\" in file.lower() or \"samsung\" in file.lower()\n",
    "#             ):\n",
    "#                 csv_path = os.path.join(subdir, file)\n",
    "#                 try:\n",
    "#                     visualize_csv(csv_path)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing file {csv_path}: {e}\")\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     root_directory = \"./data/cogwear\"\n",
    "# #     process_directory(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset wearables physiological input aggregation\n",
    "\n",
    "The aggregation process of Samsung and Empatica sourced records into the cogwear-agg.csv (output) file involves several key steps to ensure the proper structuring and representation of the data.\n",
    "\n",
    "Firstly, each row extracted from the source CSV files undergoes a mapping process to transform its contents into a new row within the output file. During this mapping process, existing values are appropriately assigned to their corresponding columns in the output file. However, if any values are missing in the source records, the respective fields in the output file are left empty to maintain the integrity of the data structure.\n",
    "\n",
    "The columns in the output file are defined as follows:\n",
    "\n",
    "1. participant_id: Each participant's folder index is translated into a participant ID to uniquely identify the source of the data.\n",
    "2. empatica_bvp: Represents the readings extracted from the Empatica device's Blood Volume Pulse (BVP) sensor.\n",
    "3. empatica_bvp_time: Corresponds to the time records associated with the Empatica BVP readings.\n",
    "4. empatica_eda: Denotes the readings obtained from the Empatica device's Electrodermal Activity (EDA) sensor.\n",
    "5. empatica_eda_time: Reflects the time records corresponding to the Empatica EDA readings.\n",
    "6. empatica_temp: Signifies the temperature readings captured by the Empatica device.\n",
    "7. empatica_temp_time: Indicates the time records linked to the Empatica temperature readings.\n",
    "8. samsung_bvp: Represents the readings collected from the Samsung device's Blood Volume Pulse (BVP) sensor.\n",
    "9. samsung_bvp_time: Corresponds to the time records associated with the Samsung BVP readings.\n",
    "10. CL: Stands for Cognitive Load and serves as a categorical indicator denoting the cognitive workload experienced during the data recording process. A value of 1 (high) indicates records sourced from the \"cognitive_load\" subdirectory, whereas a value of 0 (low) signifies records sourced from the \"baseline\" subdirectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# def process_directory(root_dir, output_file):\n",
    "#     # Create an empty list to store aggregated data\n",
    "#     agg_data = []\n",
    "\n",
    "#     # Iterate through the participant directories\n",
    "#     for participant_dir in os.listdir(root_dir):\n",
    "#         participant_id = (\n",
    "#             participant_dir  # Participant ID is the name of the subdirectory\n",
    "#         )\n",
    "\n",
    "#         # Iterate through the subdirectories (baseline and cognitive_load)\n",
    "#         for sub_dir in [\"baseline\", \"cognitive_load\"]:\n",
    "#             sub_dir_path = os.path.join(root_dir, participant_dir, sub_dir)\n",
    "\n",
    "#             # Check if the path is a directory\n",
    "#             if os.path.isdir(sub_dir_path):\n",
    "#                 # Get the cognitive load label\n",
    "#                 cl = 0 if sub_dir == \"baseline\" else 1\n",
    "\n",
    "#                 # Iterate through the files in the subdirectory\n",
    "#                 for file in os.listdir(sub_dir_path):\n",
    "#                     if file.endswith(\".csv\") and file.startswith(\n",
    "#                         (\"empatica_bvp\", \"empatica_eda\", \"empatica_temp\", \"samsung_bvp\")\n",
    "#                     ):\n",
    "#                         file_path = os.path.join(sub_dir_path, file)\n",
    "#                         df = pd.read_csv(file_path)\n",
    "#                         # Extract relevant data and append to the aggregated list\n",
    "#                         for _, row in df.iterrows():\n",
    "#                             agg_data.append(\n",
    "#                                 {\n",
    "#                                     \"participant_id\": participant_id,\n",
    "#                                     \"empatica_bvp\": row.get(\"bvp\"),\n",
    "#                                     \"empatica_bvp_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"empatica_bvp\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"empatica_eda\": row.get(\"eda\"),\n",
    "#                                     \"empatica_eda_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"empatica_eda\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"empatica_temp\": row.get(\"temp\"),\n",
    "#                                     \"empatica_temp_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"empatica_temp\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"samsung_bvp\": row.get(\"PPG GREEN\"),\n",
    "#                                     \"samsung_bvp_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"samsung_bvp\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"CL\": cl,\n",
    "#                                 }\n",
    "#                             )\n",
    "\n",
    "#     # Create a DataFrame from the aggregated data\n",
    "#     agg_df = pd.DataFrame(agg_data)\n",
    "\n",
    "#     # Write aggregated DataFrame to CSV file\n",
    "#     agg_df.to_csv(output_file, index=False)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root_directory = \"./data/cogwear/pilot\"\n",
    "#     output_file = \"./data/processed/cogwear-agg.csv\"\n",
    "#     process_directory(root_directory, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time sequenced aggregation\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "- Prepare the primary dataset for machine learning model training.\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "\n",
    "- Records sourced from participants' use of Samsung and Empatica wearables.\n",
    "- Variation in record frequency among sensors.\n",
    "\n",
    "**Processing Strategy:**\n",
    "\n",
    "- Interval Division: Divide records into 5-second intervals.\n",
    "- Data Aggregation:\n",
    "  - Calculate mean values for each column within each interval.\n",
    "  - Result: Single record representing a 5-second interval with mean values.\n",
    "- Labeling:\n",
    "  - Assign appropriate labels (1 or 0) indicating cognitive load level.\n",
    "\n",
    "**Outcome:**\n",
    "\n",
    "- Uniform dataset suitable for training machine learning models.\n",
    "- Each record corresponds to a 5-second interval with mean values and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def aggregate_by_time(df):\n",
    "    # Create an empty list to store aggregated data\n",
    "    agg_data = []\n",
    "\n",
    "    # Iterate over each participant\n",
    "    for participant_id in df[\"participant_id\"].unique():\n",
    "        participant_df = df[df[\"participant_id\"] == participant_id]\n",
    "\n",
    "        # Iterate over each time sequence\n",
    "        min_time = participant_df[\"empatica_bvp_time\"].min()\n",
    "        max_time = participant_df[\"empatica_bvp_time\"].max()\n",
    "        intervals = np.arange(min_time, max_time + 5, 5)\n",
    "\n",
    "        for interval_start, interval_end in zip(intervals[:-1], intervals[1:]):\n",
    "            interval_data = {\"participant_id\": participant_id}\n",
    "\n",
    "            for column in [\n",
    "                \"empatica_bvp\",\n",
    "                \"empatica_eda\",\n",
    "                \"empatica_temp\",\n",
    "                \"samsung_bvp\",\n",
    "            ]:\n",
    "                interval_values = participant_df[\n",
    "                    (participant_df[\"empatica_bvp_time\"] >= interval_start)\n",
    "                    & (participant_df[\"empatica_bvp_time\"] < interval_end)\n",
    "                ][column].tolist()\n",
    "                if interval_values:\n",
    "                    interval_data[column] = interval_values\n",
    "                    interval_data[column + \"_time\"] = [interval_start] * len(\n",
    "                        interval_values\n",
    "                    )\n",
    "\n",
    "            if interval_data:  # Check if interval_data is not empty\n",
    "                agg_data.append(interval_data)\n",
    "\n",
    "    # Create DataFrame from aggregated data\n",
    "    agg_df = pd.DataFrame(agg_data)\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the existing cogwear-agg.csv\n",
    "    cogwear_df = pd.read_csv(\"./data/processed/cogwear-agg.csv\")\n",
    "\n",
    "    # Aggregate the data by time sequences and participant IDs\n",
    "    aggregated_df = aggregate_by_time(cogwear_df)\n",
    "\n",
    "    # Write the aggregated data to a new CSV file\n",
    "    aggregated_df.to_csv(\"./data/processed/cogwear-agg-time-secv.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
