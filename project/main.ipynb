{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Load and Wearable Integration Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset traversal discovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def visualize_csv(csv_path):\n",
    "#     # Read CSV file\n",
    "#     df = pd.read_csv(csv_path)\n",
    "\n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for column in df.columns:\n",
    "#         if column != \"time\":\n",
    "#             plt.plot(df[\"time\"], df[column], label=column)\n",
    "#     plt.xlabel(\"Time\")\n",
    "#     plt.ylabel(\"Value\")\n",
    "#     plt.title(\"Data from {}\".format(os.path.basename(csv_path)))\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def process_directory(root_dir):\n",
    "#     for subdir, dirs, files in os.walk(root_dir):\n",
    "#         for file in files:\n",
    "#             if file.endswith(\".csv\") and (\n",
    "#                 \"empatica\" in file.lower() or \"samsung\" in file.lower()\n",
    "#             ):\n",
    "#                 csv_path = os.path.join(subdir, file)\n",
    "#                 try:\n",
    "#                     visualize_csv(csv_path)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing file {csv_path}: {e}\")\n",
    "\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     root_directory = \"./data/cogwear\"\n",
    "# #     process_directory(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset wearables physiological input aggregation\n",
    "\n",
    "The aggregation process of Samsung and Empatica sourced records into the cogwear-agg.csv (output) file involves several key steps to ensure the proper structuring and representation of the data.\n",
    "\n",
    "Firstly, each row extracted from the source CSV files undergoes a mapping process to transform its contents into a new row within the output file. During this mapping process, existing values are appropriately assigned to their corresponding columns in the output file. However, if any values are missing in the source records, the respective fields in the output file are left empty to maintain the integrity of the data structure.\n",
    "\n",
    "The columns in the output file are defined as follows:\n",
    "\n",
    "1. participant_id: Each participant's folder index is translated into a participant ID to uniquely identify the source of the data.\n",
    "2. empatica_bvp: Represents the readings extracted from the Empatica device's Blood Volume Pulse (BVP) sensor.\n",
    "3. empatica_bvp_time: Corresponds to the time records associated with the Empatica BVP readings.\n",
    "4. empatica_eda: Denotes the readings obtained from the Empatica device's Electrodermal Activity (EDA) sensor.\n",
    "5. empatica_eda_time: Reflects the time records corresponding to the Empatica EDA readings.\n",
    "6. empatica_temp: Signifies the temperature readings captured by the Empatica device.\n",
    "7. empatica_temp_time: Indicates the time records linked to the Empatica temperature readings.\n",
    "8. samsung_bvp: Represents the readings collected from the Samsung device's Blood Volume Pulse (BVP) sensor.\n",
    "9. samsung_bvp_time: Corresponds to the time records associated with the Samsung BVP readings.\n",
    "10. CL: Stands for Cognitive Load and serves as a categorical indicator denoting the cognitive workload experienced during the data recording process. A value of 1 (high) indicates records sourced from the \"cognitive_load\" subdirectory, whereas a value of 0 (low) signifies records sourced from the \"baseline\" subdirectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# def process_directory(root_dir, output_file):\n",
    "#     # Create an empty list to store aggregated data\n",
    "#     agg_data = []\n",
    "\n",
    "#     # Iterate through the participant directories\n",
    "#     for participant_dir in os.listdir(root_dir):\n",
    "#         participant_id = (\n",
    "#             participant_dir  # Participant ID is the name of the subdirectory\n",
    "#         )\n",
    "\n",
    "#         # Iterate through the subdirectories (baseline and cognitive_load)\n",
    "#         for sub_dir in [\"baseline\", \"cognitive_load\"]:\n",
    "#             sub_dir_path = os.path.join(root_dir, participant_dir, sub_dir)\n",
    "\n",
    "#             # Check if the path is a directory\n",
    "#             if os.path.isdir(sub_dir_path):\n",
    "#                 # Get the cognitive load label\n",
    "#                 cl = 0 if sub_dir == \"baseline\" else 1\n",
    "\n",
    "#                 # Iterate through the files in the subdirectory\n",
    "#                 for file in os.listdir(sub_dir_path):\n",
    "#                     if file.endswith(\".csv\") and file.startswith(\n",
    "#                         (\"empatica_bvp\", \"empatica_eda\", \"empatica_temp\", \"samsung_bvp\")\n",
    "#                     ):\n",
    "#                         file_path = os.path.join(sub_dir_path, file)\n",
    "#                         df = pd.read_csv(file_path)\n",
    "#                         # Extract relevant data and append to the aggregated list\n",
    "#                         for _, row in df.iterrows():\n",
    "#                             agg_data.append(\n",
    "#                                 {\n",
    "#                                     \"participant_id\": participant_id,\n",
    "#                                     \"empatica_bvp\": row.get(\"bvp\"),\n",
    "#                                     \"empatica_bvp_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"empatica_bvp\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"empatica_eda\": row.get(\"eda\"),\n",
    "#                                     \"empatica_eda_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"empatica_eda\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"empatica_temp\": row.get(\"temp\"),\n",
    "#                                     \"empatica_temp_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"empatica_temp\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"samsung_bvp\": row.get(\"PPG GREEN\"),\n",
    "#                                     \"samsung_bvp_time\": (\n",
    "#                                         row.get(\"time\")\n",
    "#                                         if \"samsung_bvp\" in file\n",
    "#                                         else None\n",
    "#                                     ),\n",
    "#                                     \"CL\": cl,\n",
    "#                                 }\n",
    "#                             )\n",
    "\n",
    "#     # Create a DataFrame from the aggregated data\n",
    "#     agg_df = pd.DataFrame(agg_data)\n",
    "\n",
    "#     # Write aggregated DataFrame to CSV file\n",
    "#     agg_df.to_csv(output_file, index=False)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root_directory = \"./data/cogwear/pilot\"\n",
    "#     output_file = \"./data/processed/cogwear-agg.csv\"\n",
    "#     process_directory(root_directory, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time sequenced aggregation\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "- Prepare the primary dataset for machine learning model training.\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "\n",
    "- Records sourced from participants' use of Samsung and Empatica wearables.\n",
    "- Variation in record frequency among sensors.\n",
    "\n",
    "**Processing Strategy:**\n",
    "\n",
    "- Interval Division: Divide records into 5-second intervals.\n",
    "- Data Aggregation:\n",
    "  - Calculate mean values for each column within each interval.\n",
    "  - Result: Single record representing a 5-second interval with mean values.\n",
    "- Labeling:\n",
    "  - Assign appropriate labels (1 or 0) indicating cognitive load level.\n",
    "\n",
    "**Outcome:**\n",
    "\n",
    "- Uniform dataset suitable for training machine learning models.\n",
    "- Each record corresponds to a 5-second interval with mean values and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def aggregate_by_time(df):\n",
    "#     # Create an empty list to store aggregated data\n",
    "#     agg_data = []\n",
    "\n",
    "#     # Iterate over each participant\n",
    "#     for participant_id in df[\"participant_id\"].unique():\n",
    "#         participant_df = df[df[\"participant_id\"] == participant_id]\n",
    "\n",
    "#         # Iterate over each time sequence\n",
    "#         min_time = participant_df[\"empatica_bvp_time\"].min()\n",
    "#         max_time = participant_df[\"empatica_bvp_time\"].max()\n",
    "#         intervals = np.arange(min_time, max_time + 5, 5)\n",
    "\n",
    "#         for interval_start, interval_end in zip(intervals[:-1], intervals[1:]):\n",
    "#             interval_data = {\"participant_id\": participant_id}\n",
    "\n",
    "#             for column in [\n",
    "#                 \"empatica_bvp\",\n",
    "#                 \"empatica_eda\",\n",
    "#                 \"empatica_temp\",\n",
    "#                 \"samsung_bvp\",\n",
    "#             ]:\n",
    "#                 interval_values = participant_df[\n",
    "#                     (participant_df[\"empatica_bvp_time\"] >= interval_start)\n",
    "#                     & (participant_df[\"empatica_bvp_time\"] < interval_end)\n",
    "#                 ][column].tolist()\n",
    "#                 if interval_values:\n",
    "#                     interval_data[column] = interval_values\n",
    "#                     interval_data[column + \"_time\"] = [interval_start] * len(\n",
    "#                         interval_values\n",
    "#                     )\n",
    "\n",
    "#             if interval_data:  # Check if interval_data is not empty\n",
    "#                 agg_data.append(interval_data)\n",
    "\n",
    "#     # Create DataFrame from aggregated data\n",
    "#     agg_df = pd.DataFrame(agg_data)\n",
    "#     return agg_df\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Read the existing cogwear-agg.csv\n",
    "#     cogwear_df = pd.read_csv(\"./data/processed/cogwear-agg.csv\")\n",
    "\n",
    "#     # Aggregate the data by time sequences and participant IDs\n",
    "#     aggregated_df = aggregate_by_time(cogwear_df)\n",
    "\n",
    "#     # Write the aggregated data to a new CSV file\n",
    "#     aggregated_df.to_csv(\"./data/processed/cogwear-agg-time-secv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: Read the dataset\n",
    "# df = pd.read_csv(\"./data/processed/cogwear-agg.csv\")\n",
    "\n",
    "# # Step 2: Separate the dataset based on CL categories\n",
    "# low_cl_df = df[df[\"CL\"] == 0]\n",
    "# high_cl_df = df[df[\"CL\"] == 1]\n",
    "\n",
    "# # Step 3: Further split the dataset for each participant\n",
    "# low_cl_participant_dfs = {}\n",
    "# high_cl_participant_dfs = {}\n",
    "\n",
    "# for participant_id, group_df in low_cl_df.groupby(\"participant_id\"):\n",
    "#     low_cl_participant_dfs[participant_id] = group_df\n",
    "\n",
    "# for participant_id, group_df in high_cl_df.groupby(\"participant_id\"):\n",
    "#     high_cl_participant_dfs[participant_id] = group_df\n",
    "\n",
    "# # Step 4: Save the low and high cognitive load datasets for each participant into separate directories / CSV files\n",
    "# for participant_id, df in low_cl_participant_dfs.items():\n",
    "#     df.to_csv(\n",
    "#         f\"./data/participant-division/{participant_id}/{participant_id}_LOW_CL.csv\",\n",
    "#         index=False,\n",
    "#     )\n",
    "\n",
    "# for participant_id, df in high_cl_participant_dfs.items():\n",
    "#     df.to_csv(\n",
    "#         f\"./data/participant-division/{participant_id}/{participant_id}_HIGH_CL.csv\",\n",
    "#         index=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform original records into new, time-sequenced records with 5-second intervals and mean values.\n",
    "\n",
    "### Data Preparation:\n",
    "\n",
    "- Read the original dataset containing sensor readings and cognitive load labels.\n",
    "- Convert time columns to datetime objects for proper time handling.\n",
    "- Sort the dataset by time columns to ensure sequential ordering.\n",
    "\n",
    "### Define & Create Time Windows:\n",
    "\n",
    "- Define the size of the time windows, typically set to 5 seconds.\n",
    "- Combine time columns from all sensors to create a unified time column.\n",
    "- Group the data into 5-second intervals using the combined time column.\n",
    "\n",
    "### Process Each Time Window:\n",
    "\n",
    "- Iterate over each 5-second interval.\n",
    "- Calculate the mean values for sensor readings (bvp, eda, temp) within the interval.\n",
    "- Calculate the upper limit of the time window to represent the end time of the interval.\n",
    "\n",
    "### Generate New Records:\n",
    "\n",
    "- For each interval, create a new record containing:\n",
    "  - Participant ID: Identifies the participant associated with the record.\n",
    "  - Mean sensor values: Calculated mean values for each sensor reading within the interval.\n",
    "  - Time: Represents the upper limit of the time window, indicating the end time of the interval.\n",
    "  - Cognitive Load (CL): Indicates the cognitive load level associated with the interval (0 for low, 1 for high).\n",
    "\n",
    "### Aggregate New Records:\n",
    "\n",
    "- Store the new records in a DataFrame or CSV file for further analysis or modeling.\n",
    "\n",
    "### Repeat for Each Participant and Cognitive Load Level:\n",
    "\n",
    "- Iterate through each participant's data and perform the above steps separately for low and high cognitive load sessions.\n",
    "\n",
    "### Save Results:\n",
    "\n",
    "- Save the processed data into separate files or databases for future use or analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Function to process the data and create aggregated records\n",
    "# def process_data(df, cl_type):\n",
    "#     # Convert time columns to datetime objects\n",
    "#     df[\"empatica_bvp_time\"] = pd.to_datetime(df[\"empatica_bvp_time\"], unit=\"s\")\n",
    "#     df[\"empatica_eda_time\"] = pd.to_datetime(df[\"empatica_eda_time\"], unit=\"s\")\n",
    "#     df[\"empatica_temp_time\"] = pd.to_datetime(df[\"empatica_temp_time\"], unit=\"s\")\n",
    "#     df[\"samsung_bvp_time\"] = pd.to_datetime(df[\"samsung_bvp_time\"], unit=\"s\")\n",
    "\n",
    "#     # Sort the DataFrame by time columns\n",
    "#     df.sort_values(\n",
    "#         by=[\n",
    "#             \"empatica_bvp_time\",\n",
    "#             \"empatica_eda_time\",\n",
    "#             \"empatica_temp_time\",\n",
    "#             \"samsung_bvp_time\",\n",
    "#         ],\n",
    "#         inplace=True,\n",
    "#     )\n",
    "\n",
    "#     # Define the time window size (in seconds)\n",
    "#     time_window_size = 5\n",
    "\n",
    "#     # Create a combined time column representing the maximum of all time columns\n",
    "#     df[\"time\"] = df[\n",
    "#         [\n",
    "#             \"empatica_bvp_time\",\n",
    "#             \"empatica_eda_time\",\n",
    "#             \"empatica_temp_time\",\n",
    "#             \"samsung_bvp_time\",\n",
    "#         ]\n",
    "#     ].max(axis=1)\n",
    "\n",
    "#     # Group the data into 5-second intervals\n",
    "#     grouped = df.groupby(pd.Grouper(key=\"time\", freq=f\"{time_window_size}S\"))\n",
    "\n",
    "#     # Create a new DataFrame to store the aggregated records\n",
    "#     new_df = pd.DataFrame(\n",
    "#         columns=[\n",
    "#             \"ID\",\n",
    "#             \"empatica_bvp\",\n",
    "#             \"empatica_eda\",\n",
    "#             \"empatica_temp\",\n",
    "#             \"samsung_bvp\",\n",
    "#             \"time\",\n",
    "#             \"CL\",\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # Iterate over each 5-second interval\n",
    "#     for time, group in grouped:\n",
    "#         # Calculate the mean values for each column\n",
    "#         mean_values = group.mean()\n",
    "\n",
    "#         # Calculate the upper limit of the time window\n",
    "#         time_upper_limit = time + pd.Timedelta(seconds=time_window_size)\n",
    "\n",
    "#         # Append the mean values to the new DataFrame\n",
    "#         new_df = new_df.append(\n",
    "#             {\n",
    "#                 \"ID\": mean_values[\"participant_id\"],\n",
    "#                 \"empatica_bvp\": mean_values[\"empatica_bvp\"],\n",
    "#                 \"empatica_eda\": mean_values[\"empatica_eda\"],\n",
    "#                 \"empatica_temp\": mean_values[\"empatica_temp\"],\n",
    "#                 \"samsung_bvp\": mean_values[\"samsung_bvp\"],\n",
    "#                 \"time\": time_upper_limit,\n",
    "#                 \"CL\": cl_type,\n",
    "#             },\n",
    "#             ignore_index=True,\n",
    "#         )\n",
    "\n",
    "#     return new_df\n",
    "\n",
    "\n",
    "# # Define the directory containing participant folders\n",
    "# participant_directory = \"./data/participant-division/\"\n",
    "\n",
    "# # Iterate through each participant folder\n",
    "# for participant_id in os.listdir(participant_directory):\n",
    "#     participant_folder = os.path.join(participant_directory, participant_id)\n",
    "\n",
    "#     # Iterate through low and high cognitive load files\n",
    "#     for cl_type in [\"LOW_CL\", \"HIGH_CL\"]:\n",
    "#         input_filename = f\"{participant_id}_{cl_type}.csv\"\n",
    "#         output_filename = f\"{participant_id}_{cl_type}_SEQ.csv\"\n",
    "\n",
    "#         # Read the CSV file into a DataFrame\n",
    "#         df = pd.read_csv(os.path.join(participant_folder, input_filename))\n",
    "\n",
    "#         # Process the data and create aggregatLOWed records\n",
    "#         new_df = process_data(df, 1 if cl_type == \"HIGH_CL\" else 0)\n",
    "\n",
    "#         # Write the aggregated records to a new CSV file\n",
    "#         new_df.to_csv(os.path.join(participant_folder, output_filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequenced Datasets Aggregation\n",
    "\n",
    "Organizing the aggregated data into separate files based on the cognitive load level (0 - LOW / 1 - HIGH)\n",
    "\n",
    "- Write the aggregated records for each participant and cognitive load level to new CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing participant folders\n",
    "participant_directory = \"./data/participant-division/\"\n",
    "\n",
    "# Initialize DataFrames to store aggregated data for low and high cognitive load\n",
    "low_cl_agg_df = pd.DataFrame()\n",
    "high_cl_agg_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each participant folder\n",
    "for participant_id in os.listdir(participant_directory):\n",
    "    participant_folder = os.path.join(participant_directory, participant_id)\n",
    "\n",
    "    # Read the LOW_CL_SEQ.csv file into a DataFrame\n",
    "    low_cl_seq_filename = f\"{participant_id}_LOW_CL_SEQ.csv\"\n",
    "    low_cl_seq_path = os.path.join(participant_folder, low_cl_seq_filename)\n",
    "    if os.path.exists(low_cl_seq_path):\n",
    "        low_cl_seq_df = pd.read_csv(low_cl_seq_path)\n",
    "        # Append to the low cognitive load aggregated DataFrame\n",
    "        low_cl_agg_df = low_cl_agg_df.append(low_cl_seq_df, ignore_index=True)\n",
    "\n",
    "    # Read the HIGH_CL_SEQ.csv file into a DataFrame\n",
    "    high_cl_seq_filename = f\"{participant_id}_HIGH_CL_SEQ.csv\"\n",
    "    high_cl_seq_path = os.path.join(participant_folder, high_cl_seq_filename)\n",
    "    if os.path.exists(high_cl_seq_path):\n",
    "        high_cl_seq_df = pd.read_csv(high_cl_seq_path)\n",
    "        # Append to the high cognitive load aggregated DataFrame\n",
    "        high_cl_agg_df = high_cl_agg_df.append(high_cl_seq_df, ignore_index=True)\n",
    "\n",
    "# Save the aggregated data to new CSV files\n",
    "low_cl_agg_df.to_csv(\"./data/high_low_separation/LOW_CL_AGG.csv\", index=False)\n",
    "high_cl_agg_df.to_csv(\"./data/high_low_separation/HIGH_CL_AGG.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved at ./data/processed_dataset/cogwear_processed_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Files path definition\n",
    "high_cl_path = \"./data/high_low_separation/HIGH_CL_AGG.csv\"\n",
    "low_cl_path = \"./data/high_low_separation/LOW_CL_AGG.csv\"\n",
    "final_dataset_path = \"./data/processed_dataset/cogwear_processed_dataset.csv\"\n",
    "\n",
    "\n",
    "def merge_high_low_cl_files(file1_path, file2_path, output_path):\n",
    "    # Read the two CSV files into DataFrames\n",
    "    df1 = pd.read_csv(file1_path)\n",
    "    df2 = pd.read_csv(file2_path)\n",
    "\n",
    "    # Merge the two DataFrames\n",
    "    merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Write the merged DataFrame to a new CSV file\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"Merged CSV file saved at {output_path}\")\n",
    "\n",
    "\n",
    "merge_high_low_cl_files(high_cl_path, low_cl_path, final_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
